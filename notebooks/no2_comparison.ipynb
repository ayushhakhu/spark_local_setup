{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c12d7f-1100-42a7-8c87-10318c1df884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/13 09:39:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"No2 Comparison\")\\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.cores.max\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d295c6ee-e19a-4d2f-913e-dce325ef5419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>No2 Comparison</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7395e13cafa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60e6706-4b35-4cc9-bc68-49aa7315e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "no2_df = spark.read.option(\"header\", True)\\\n",
    "                   .option(\"inferSchema\", True)\\\n",
    "                   .load(format='csv', path=\"/opt/spark-data/input/pakistan_no2_2025_full_year.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3250b270-4f95-4178-8c47-6f0a06541d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- no2_column: double (nullable = true)\n",
      " |-- no2_quality: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28854f4-4100-4bb8-b9c7-814df10b7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee84deab-24fd-4308-bba4-88f59e2aab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum, lag, expr\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy(\"year\").orderBy(\"date\").rowsBetween(-2, 0)\n",
    "\n",
    "no2_moving_averages = no2_df.withColumn(\"Last Three Day Average\", avg(\"no2_column\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f162987a-6723-4924-a2d0-753ae1ac4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_moving_averages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c43a19-6807-4813-9951-188c93c2b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Window [avg(no2_column#20) windowspecdefinition(year#22, date#17 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS Last Three Day Average#36], [year#22], [date#17 ASC NULLS FIRST]\n",
      "   +- Sort [year#22 ASC NULLS FIRST, date#17 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(year#22, 200), ENSURE_REQUIREMENTS, [plan_id=27]\n",
      "         +- FileScan csv [date#17,latitude#18,longitude#19,no2_column#20,no2_quality#21,year#22,month#23,day#24,day_of_week#25] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark-data/input/pakistan_no2_2025_full_year.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:date,latitude:double,longitude:double,no2_column:double,no2_quality:double,year:int,m...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no2_moving_averages.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07cf87c-8d6c-4b3c-a650-88ef636c4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_moving_averages.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f3c3d3-6473-47d5-8268-947365e4ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8333f9c8-e744-425e-9ce9-948ba824611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "no2_moving_averages = no2_moving_averages.withColumns({\"latitude\": F.round(no2_df.latitude), \"longitude\": F.round(no2_df.longitude)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7128a6b-4de9-40b8-9d33-80e22d7d6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_moving_averages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc92a8d-6cac-41d8-acc7-23c9d7e7723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [date#17, round(latitude#18, 0) AS latitude#48, round(longitude#19, 0) AS longitude#49, no2_column#20, no2_quality#21, year#22, month#23, day#24, day_of_week#25, Last Three Day Average#36]\n",
      "   +- Window [avg(no2_column#20) windowspecdefinition(year#22, date#17 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS Last Three Day Average#36], [year#22], [date#17 ASC NULLS FIRST]\n",
      "      +- Sort [year#22 ASC NULLS FIRST, date#17 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(year#22, 200), ENSURE_REQUIREMENTS, [plan_id=42]\n",
      "            +- FileScan csv [date#17,latitude#18,longitude#19,no2_column#20,no2_quality#21,year#22,month#23,day#24,day_of_week#25] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark-data/input/pakistan_no2_2025_full_year.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:date,latitude:double,longitude:double,no2_column:double,no2_quality:double,year:int,m...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no2_moving_averages.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf7e876-5244-420e-8904-c9618a8edb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_moving_averages.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581c0171-e63a-4e88-bd83-54747746b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_repartiotned =no2_moving_averages.repartition(no2_moving_averages.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd4c8ed6-592d-48f3-b0e4-db42c57abc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_repartiotned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c0b3456-0eb0-4c33-8ab4-be045e6a9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "no2_repartiotned.write.mode(\"overwrite\").csv(\"/opt/spark-output/result\", header=True)\n",
    "# df.write.mode(\"overwrite\").csv(\"/opt/spark-data/output/result\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715d4a8-88f5-42da-ac59-84d5e5642bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_repartiotned.write.mode(\"overwrite\").csv(\"/opt/spark-output/result\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e6bc20-ed03-406a-9b61-7bf8c1d20bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange hashpartitioning(longitude#49, 200), REPARTITION_BY_COL, [plan_id=185]\n",
      "   +- Project [date#17, round(latitude#18, 0) AS latitude#48, round(longitude#19, 0) AS longitude#49, no2_column#20, no2_quality#21, year#22, month#23, day#24, day_of_week#25, Last Three Day Average#36]\n",
      "      +- Window [avg(no2_column#20) windowspecdefinition(year#22, date#17 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) AS Last Three Day Average#36], [year#22], [date#17 ASC NULLS FIRST]\n",
      "         +- Sort [year#22 ASC NULLS FIRST, date#17 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(year#22, 200), ENSURE_REQUIREMENTS, [plan_id=181]\n",
      "               +- FileScan csv [date#17,latitude#18,longitude#19,no2_column#20,no2_quality#21,year#22,month#23,day#24,day_of_week#25] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark-data/input/pakistan_no2_2025_full_year.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:date,latitude:double,longitude:double,no2_column:double,no2_quality:double,year:int,m...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no2_repartiotned.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89d556-a8d7-4182-8717-86db87907707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
